# -*- coding: utf-8 -*-
"""battle.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DSzlq0-ETmOkJBX1FUs28EiNy_qlh0XV
"""



import json
import requests

# URL of the dataset
dataset_url = "https://raw.githubusercontent.com/MathuraMG/indian-history/refs/heads/master/battles.json"

# Download and load the dataset
response = requests.get(dataset_url)
battles_data = response.json()

# Display an example battle
print("Sample Battle Data:", battles_data[0])

# Let's inspect the first item of the data to see its structure
print("Data preview:", battles_data[0])

{
  'name': 'Battle of Plassey',
  'year': 1757,
  'participants': ['British East India Company', 'Nawab of Bengal'],
  'description': 'A decisive victory for the British...',
  'outcome': 'British victory'
}

train_text = "\n\n".join([
    f"Battle: {b['name']}\nYear: {b['year']}\nDescription: {b['description']}\nOutcome: {b['outcome']}"
    for b in battles_data if all(k in b for k in ['name', 'year', 'description', 'outcome'])
])

# Save to file
with open("battles_train.txt", "w", encoding="utf-8") as f:
    f.write(train_text)

# Preview the training data
print("Sample data:\n", train_text[:500])



print(len(battles_data))  # Should show number of entries
print(battles_data[0])    # Show the first item

train_text = "\n\n".join([
    f"Battle: {b.get('name', 'Unknown')}\nYear: {b.get('year', 'N/A')}\nDescription: {b.get('description', '')}\nOutcome: {b.get('outcome', '')}"
    for b in battles_data
])

with open("battles_train.txt", "w", encoding="utf-8") as f:
    f.write(train_text)

# Confirm content


from dataset import load_dataset

dataset = load_dataset("text", data_files={"train": "battles_train.txt"})

from dataset import Dataset, Features, Value

with open("battles_train.txt", "r", encoding="utf-8") as f:
    lines = f.read().split("\n\n")  # Each battle is a separate example

data = [{"text": line} for line in lines if line.strip() != ""]

features = Features({"text": Value("string")})
dataset = Dataset.from_list(data, features=features)

from dataset import load_dataset

dataset = load_dataset("text", data_files={"train": "battles_train.txt"})

from transformers import GPT2Tokenizer, GPT2LMHeadModel
from dataset import load_dataset
from transformers import DataCollatorForLanguageModeling

# Load tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.pad_token = tokenizer.eos_token  # Fix padding issue
model = GPT2LMHeadModel.from_pretrained("gpt2")

# Load your custom dataset
dataset = load_dataset("text", data_files={"train": "battles_train.txt"})

# Tokenize it
def tokenize_function(examples):
    return tokenizer(examples["text"], padding="max_length", truncation=True, max_length=512)

tokenized_dataset = dataset.map(tokenize_function, batched=True)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

from torch.utils.data import Dataset, DataLoader
import torch

# Create a custom dataset class
class BattlesDataset(Dataset):
    def __init__(self, text, tokenizer, max_length=512):
        self.tokenizer = tokenizer
        self.input_ids = tokenizer(text, truncation=True, padding="max_length", max_length=max_length, return_tensors="pt")["input_ids"]

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {"input_ids": self.input_ids[idx]}

# Tokenize datasetfrom fastapi import FastAPI
dataset = BattlesDataset(train_text, tokenizer)

# Create DataLoader
train_loader = DataLoader(dataset, batch_size=2, shuffle=True)

print("Dataset Ready! Number of samples:", len(dataset))

# Save fine-tuned model
model.save_pretrained("./gpt2_battles")
tokenizer.save_pretrained("./gpt2_battles")

print("Model Saved Successfully!")

from transformers import GPT2LMHeadModel, GPT2Tokenizer

# Load trained model
model = GPT2LMHeadModel.from_pretrained("./gpt2_battles")
tokenizer = GPT2Tokenizer.from_pretrained("./gpt2_battles")

print("Fine-Tuned GPT-2 Model Loaded!")

import torch

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Generate battle story
input_text = "Battle of Panipat"
input_ids = tokenizer.encode(input_text, return_tensors="pt").to(device)

# Generate continuation
output = model.generate(
    input_ids,
    max_length=150,
    temperature=0.7,
    num_return_sequences=1,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    pad_token_id=tokenizer.eos_token_id  # To avoid warning
)

# Decode and print result
generated_text = tokenizer.decode(output[0], skip_special_tokens=True)
print("\nGenerated Story:\n", generated_text)

# main.py
from fastapi import FastAPI, Request
from pydantic import BaseModel
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import torch

# Load model and tokenizer
model_path = "./gpt2_battles"  # Change to your saved model path
tokenizer = GPT2Tokenizer.from_pretrained(model_path)
model = GPT2LMHeadModel.from_pretrained(model_path)

# Ensure pad_token is set
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# FastAPI setup
app = FastAPI()

# Input schema
class Prompt(BaseModel):
    text: str

# Generate endpoint
@app.post("/generate/")
def generate_text(prompt: Prompt):
    input_ids = tokenizer.encode(prompt.text, return_tensors="pt").to(device)

    output = model.generate(
        input_ids,
        max_length=200,
        temperature=0.7,
        num_return_sequences=1,
        top_p=0.95,
        top_k=50,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )

    generated = tokenizer.decode(output[0], skip_special_tokens=True)
    return {"generated_text": generated}


